{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b29908d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>262696992176304465</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@Oirisheye Hey you! I'm gonna be in Dublin in ...</td>\n",
       "      <td>usermention hey gon na dublin february know sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>410734138242126311</td>\n",
       "      <td>positive</td>\n",
       "      <td>Literally so excited I'm going to a Sam Smith ...</td>\n",
       "      <td>literally excited going sam smith concert october</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>237615985571058688</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@WINDmobile Will there be an option to buy the...</td>\n",
       "      <td>usermention option buy gb ram model moto rd ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90473590077188360</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Our Little Ms. Philippines. üëèüëèüëè#littleMsPhilip...</td>\n",
       "      <td>little ms philippines hashtag hashtag co kqk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>450236582392850660</td>\n",
       "      <td>negative</td>\n",
       "      <td>@AngryRaiderFan I know. This, TPP, expanded wa...</td>\n",
       "      <td>usermention know tpp expanded wars drone strik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id     label  \\\n",
       "0  262696992176304465   neutral   \n",
       "1  410734138242126311  positive   \n",
       "2  237615985571058688   neutral   \n",
       "3   90473590077188360   neutral   \n",
       "4  450236582392850660  negative   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  @Oirisheye Hey you! I'm gonna be in Dublin in ...   \n",
       "1  Literally so excited I'm going to a Sam Smith ...   \n",
       "2  @WINDmobile Will there be an option to buy the...   \n",
       "3  Our Little Ms. Philippines. üëèüëèüëè#littleMsPhilip...   \n",
       "4  @AngryRaiderFan I know. This, TPP, expanded wa...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  usermention hey gon na dublin february know sa...  \n",
       "1  literally excited going sam smith concert october  \n",
       "2  usermention option buy gb ram model moto rd ge...  \n",
       "3       little ms philippines hashtag hashtag co kqk  \n",
       "4  usermention know tpp expanded wars drone strik...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re,string,random\n",
    "import torchtext\n",
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "#For preprocessing of text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "#For building classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dictionary is taken from kaggle\n",
    "contractions_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                    \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n",
    "                    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
    "                    \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                    \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \n",
    "                    \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                    \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                    \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                    \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "                    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n",
    "                    \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                    \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n",
    "                    \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
    "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
    "                    \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n",
    "                    \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \n",
    "                    \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "                    \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
    "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n",
    "                    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
    "                    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                    \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                    \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "def convert_contraction_to_original(tweet):\n",
    "    if type(tweet) is str:\n",
    "        for key in contractions_dict:\n",
    "            value = contractions_dict[key]\n",
    "            tweet = tweet.replace(key, value)\n",
    "        return tweet\n",
    "    else:\n",
    "        return tweet\n",
    "    \n",
    "def dataset_load(filepath):\n",
    "    df = pd.read_csv(filepath, sep='\\t', header=None)\n",
    "    return df\n",
    "\n",
    "def preprocess_tweets(tweet):\n",
    "    \n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()    \n",
    "    \n",
    "    #Remove usermentions\n",
    "    r = re.findall(\"@[\\w]*\", tweet)\n",
    "    for word in r:\n",
    "        tweet = re.sub(word, \"usermention\", tweet)\n",
    "    \n",
    "    #remove hashtags\n",
    "    tweet = re.sub(r'#([a-z0-9]+)(?:\\b|$)', \"hashtag\", tweet)    \n",
    "    \n",
    "    # remove special characters, numbers and punctuations\n",
    "    tweet = re.sub(r\"[^a-zA-Z]\", \" \", tweet)\n",
    "    \n",
    "    #remove shortwords\n",
    "    tweet = \" \".join([w for w in tweet.split() if len(w)>1])\n",
    "    \n",
    "    #Remove urls\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", tweet, flags = re.MULTILINE)\n",
    "    \n",
    "    #Remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    filtered_words = [word for word in tweet_tokens if word not in stopwords_english]\n",
    "    \n",
    "    #Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in filtered_words]\n",
    "    \n",
    "    return \" \". join(lemma_words)\n",
    "\n",
    "df = dataset_load(r\"C:/Users/harsh/Downloads/Warwick/NLP/Assignment/semeval-tweets.tar/semeval-tweets/twitter-dev-data.txt\")\n",
    "df = df.rename({0: 'Id', 1: 'label' , 2: 'tweet'}, axis='columns')\n",
    "\n",
    "df['clean_tweet'] = np.vectorize(convert_contraction_to_original)(df['tweet'])\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: preprocess_tweets(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c076469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\anaconda3\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neutral': 0, 'positive': 1, 'negative': 2})\n"
     ]
    }
   ],
   "source": [
    "SEED = 2021\n",
    "\n",
    "#Torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "#Cuda algorithms\n",
    "torch.backends.cudnn.deterministic = True  \n",
    "\n",
    "df1 = df[df.clean_tweet != '']\n",
    "df1.to_csv('df.csv',index=False)\n",
    "\n",
    "TEXT = torchtext.legacy.data.Field(tokenize = 'spacy',batch_first=True,include_lengths=True)\n",
    "LABEL = torchtext.legacy.data.LabelField(dtype=torch.float,batch_first=True)\n",
    "fields = [('Id', None),('label', LABEL),('tweet',None),('clean_tweet', TEXT)]\n",
    "\n",
    "training_data=torchtext.legacy.data.TabularDataset(path = 'df.csv',format = 'csv',fields = fields,skip_header = True)\n",
    "\n",
    "train_data, valid_data = training_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = 4998, min_freq=3,vectors = \"glove.6B.100d\")  \n",
    "LABEL.build_vocab(train_data)\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9dd1a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "classifier(\n",
      "  (embedding): Embedding(1194, 100)\n",
      "  (lstm): LSTM(100, 32, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (act): Sigmoid()\n",
      ")\n",
      "The model has 145,000 trainable parameters\n",
      "torch.Size([1194, 100])\n"
     ]
    }
   ],
   "source": [
    "#check whether cuda is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "\n",
    "#set batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#Load an iterator\n",
    "train_iterator, valid_iterator = torchtext.legacy.data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.clean_tweet),\n",
    "    sort_within_batch=True,\n",
    "    device = device)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class classifier(nn.Module):\n",
    "    \n",
    "    #define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        \n",
    "        #Constructor\n",
    "        super().__init__()          \n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        #dense layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        for param in self.fc.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.fc.eval()\n",
    "            #print(self.fc.weight.requires_grad)\n",
    "        \n",
    "        #activation function\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [batch size,sent_length]\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent_len, emb dim]\n",
    "      \n",
    "        #packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        #hidden = [batch size, num layers * num directions,hid dim]\n",
    "        #cell = [batch size, num layers * num directions,hid dim]\n",
    "        \n",
    "        #concat the final forward and backward hidden state\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        dense_outputs=self.fc(hidden)\n",
    "\n",
    "        #Final activation function\n",
    "        outputs=self.act(dense_outputs)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "#define hyperparameters\n",
    "size_of_vocab = len(TEXT.vocab)\n",
    "embedding_dim = 100\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = 1\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "#instantiate the model\n",
    "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \n",
    "                   bidirectional = False, dropout = dropout)\n",
    "\n",
    "#architecture\n",
    "print(model)\n",
    "\n",
    "#No. of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "#Initialize the pretrained embedding\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a83ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.005)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "#define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    \n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "    \n",
    "#push to cuda if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7080e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    #initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    #set the model in training phase\n",
    "    model.train()  \n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        #resets the gradients after every batch\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        #retrieve text and no. of words\n",
    "        text, text_lengths = batch.clean_tweet   \n",
    "        \n",
    "        #convert to 1D tensor\n",
    "        predictions = model(text, text_lengths).squeeze(1)  \n",
    "        \n",
    "        #compute the loss\n",
    "        loss = criterion(predictions, batch.label)        \n",
    "        \n",
    "        #compute the binary accuracy\n",
    "        acc = binary_accuracy(predictions, batch.label)   \n",
    "        \n",
    "        #backpropage the loss and compute the gradients\n",
    "        loss.backward()       \n",
    "        \n",
    "        #update the weights\n",
    "        optimizer.step()      \n",
    "        \n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item()  \n",
    "        epoch_acc += acc.item()    \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    #deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    #deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "        \n",
    "            #retrieve text and no. of words\n",
    "            text, text_lengths = batch.clean_tweet\n",
    "            \n",
    "            #convert to 1d tensor\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd64192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.534 | Train Acc: 36.08%\n",
      "\t Val. Loss: 0.639 |  Val. Acc: 33.28%\n",
      "\tTrain Loss: 0.404 | Train Acc: 41.33%\n",
      "\t Val. Loss: 0.672 |  Val. Acc: 41.04%\n",
      "\tTrain Loss: 0.181 | Train Acc: 46.84%\n",
      "\t Val. Loss: 0.715 |  Val. Acc: 43.33%\n",
      "\tTrain Loss: 0.027 | Train Acc: 54.91%\n",
      "\t Val. Loss: 0.580 |  Val. Acc: 48.23%\n",
      "\tTrain Loss: -0.119 | Train Acc: 59.20%\n",
      "\t Val. Loss: 0.791 |  Val. Acc: 43.49%\n",
      "\tTrain Loss: -0.262 | Train Acc: 63.27%\n",
      "\t Val. Loss: 0.674 |  Val. Acc: 49.69%\n",
      "\tTrain Loss: -0.315 | Train Acc: 66.16%\n",
      "\t Val. Loss: 0.838 |  Val. Acc: 46.72%\n",
      "\tTrain Loss: -0.316 | Train Acc: 65.18%\n",
      "\t Val. Loss: 0.793 |  Val. Acc: 46.93%\n",
      "\tTrain Loss: -0.357 | Train Acc: 65.62%\n",
      "\t Val. Loss: 0.675 |  Val. Acc: 49.90%\n",
      "\tTrain Loss: -0.453 | Train Acc: 69.13%\n",
      "\t Val. Loss: 0.761 |  Val. Acc: 48.07%\n",
      "\tTrain Loss: -0.517 | Train Acc: 71.03%\n",
      "\t Val. Loss: 0.825 |  Val. Acc: 46.98%\n",
      "\tTrain Loss: -0.519 | Train Acc: 70.74%\n",
      "\t Val. Loss: 0.862 |  Val. Acc: 45.47%\n",
      "\tTrain Loss: -0.552 | Train Acc: 72.39%\n",
      "\t Val. Loss: 0.825 |  Val. Acc: 48.75%\n",
      "\tTrain Loss: -0.590 | Train Acc: 73.03%\n",
      "\t Val. Loss: 0.868 |  Val. Acc: 47.92%\n",
      "\tTrain Loss: -0.617 | Train Acc: 74.20%\n",
      "\t Val. Loss: 0.873 |  Val. Acc: 47.81%\n",
      "\tTrain Loss: -0.646 | Train Acc: 74.57%\n",
      "\t Val. Loss: 0.867 |  Val. Acc: 47.76%\n",
      "\tTrain Loss: -0.667 | Train Acc: 75.92%\n",
      "\t Val. Loss: 0.874 |  Val. Acc: 48.12%\n",
      "\tTrain Loss: -0.649 | Train Acc: 75.25%\n",
      "\t Val. Loss: 0.961 |  Val. Acc: 45.57%\n",
      "\tTrain Loss: -0.618 | Train Acc: 73.97%\n",
      "\t Val. Loss: 0.835 |  Val. Acc: 48.39%\n",
      "\tTrain Loss: -0.669 | Train Acc: 75.54%\n",
      "\t Val. Loss: 0.903 |  Val. Acc: 47.19%\n",
      "\tTrain Loss: -0.675 | Train Acc: 75.68%\n",
      "\t Val. Loss: 0.908 |  Val. Acc: 45.68%\n",
      "\tTrain Loss: -0.720 | Train Acc: 76.88%\n",
      "\t Val. Loss: 0.990 |  Val. Acc: 48.44%\n",
      "\tTrain Loss: -0.742 | Train Acc: 77.33%\n",
      "\t Val. Loss: 1.010 |  Val. Acc: 47.81%\n",
      "\tTrain Loss: -0.749 | Train Acc: 77.51%\n",
      "\t Val. Loss: 1.000 |  Val. Acc: 45.99%\n",
      "\tTrain Loss: -0.766 | Train Acc: 77.90%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 48.39%\n",
      "\tTrain Loss: -0.778 | Train Acc: 78.42%\n",
      "\t Val. Loss: 1.059 |  Val. Acc: 47.08%\n",
      "\tTrain Loss: -0.788 | Train Acc: 78.34%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 48.59%\n",
      "\tTrain Loss: -0.788 | Train Acc: 78.62%\n",
      "\t Val. Loss: 1.076 |  Val. Acc: 48.39%\n",
      "\tTrain Loss: -0.797 | Train Acc: 78.83%\n",
      "\t Val. Loss: 1.084 |  Val. Acc: 46.88%\n",
      "\tTrain Loss: -0.807 | Train Acc: 79.10%\n",
      "\t Val. Loss: 1.112 |  Val. Acc: 47.45%\n",
      "\tTrain Loss: -0.808 | Train Acc: 78.84%\n",
      "\t Val. Loss: 1.037 |  Val. Acc: 49.22%\n",
      "\tTrain Loss: -0.759 | Train Acc: 77.64%\n",
      "\t Val. Loss: 1.069 |  Val. Acc: 45.52%\n",
      "\tTrain Loss: -0.767 | Train Acc: 77.60%\n",
      "\t Val. Loss: 1.007 |  Val. Acc: 48.44%\n",
      "\tTrain Loss: -0.769 | Train Acc: 77.82%\n",
      "\t Val. Loss: 1.008 |  Val. Acc: 47.71%\n",
      "\tTrain Loss: -0.792 | Train Acc: 78.61%\n",
      "\t Val. Loss: 0.943 |  Val. Acc: 47.55%\n",
      "\tTrain Loss: -0.693 | Train Acc: 75.95%\n",
      "\t Val. Loss: 1.202 |  Val. Acc: 41.67%\n",
      "\tTrain Loss: -0.599 | Train Acc: 74.28%\n",
      "\t Val. Loss: 1.037 |  Val. Acc: 45.57%\n",
      "\tTrain Loss: -0.729 | Train Acc: 76.98%\n",
      "\t Val. Loss: 1.081 |  Val. Acc: 48.18%\n",
      "\tTrain Loss: -0.719 | Train Acc: 77.51%\n",
      "\t Val. Loss: 0.989 |  Val. Acc: 47.40%\n",
      "\tTrain Loss: -0.750 | Train Acc: 77.97%\n",
      "\t Val. Loss: 1.013 |  Val. Acc: 47.45%\n",
      "\tTrain Loss: -0.796 | Train Acc: 78.90%\n",
      "\t Val. Loss: 1.036 |  Val. Acc: 48.80%\n",
      "\tTrain Loss: -0.801 | Train Acc: 79.07%\n",
      "\t Val. Loss: 1.045 |  Val. Acc: 47.14%\n",
      "\tTrain Loss: -0.815 | Train Acc: 79.20%\n",
      "\t Val. Loss: 1.057 |  Val. Acc: 47.55%\n",
      "\tTrain Loss: -0.820 | Train Acc: 79.33%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 47.71%\n",
      "\tTrain Loss: -0.823 | Train Acc: 79.41%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 46.82%\n",
      "\tTrain Loss: -0.824 | Train Acc: 79.35%\n",
      "\t Val. Loss: 1.108 |  Val. Acc: 46.67%\n",
      "\tTrain Loss: -0.826 | Train Acc: 79.49%\n",
      "\t Val. Loss: 1.126 |  Val. Acc: 46.98%\n",
      "\tTrain Loss: -0.829 | Train Acc: 79.48%\n",
      "\t Val. Loss: 1.122 |  Val. Acc: 46.67%\n",
      "\tTrain Loss: -0.831 | Train Acc: 79.39%\n",
      "\t Val. Loss: 1.136 |  Val. Acc: 46.82%\n",
      "\tTrain Loss: -0.831 | Train Acc: 79.42%\n",
      "\t Val. Loss: 1.141 |  Val. Acc: 46.41%\n",
      "\tTrain Loss: -0.833 | Train Acc: 79.56%\n",
      "\t Val. Loss: 1.157 |  Val. Acc: 45.94%\n",
      "\tTrain Loss: -0.835 | Train Acc: 79.55%\n",
      "\t Val. Loss: 1.144 |  Val. Acc: 46.82%\n",
      "\tTrain Loss: -0.837 | Train Acc: 79.56%\n",
      "\t Val. Loss: 1.164 |  Val. Acc: 46.51%\n",
      "\tTrain Loss: -0.836 | Train Acc: 79.58%\n",
      "\t Val. Loss: 1.183 |  Val. Acc: 46.35%\n",
      "\tTrain Loss: -0.839 | Train Acc: 79.55%\n",
      "\t Val. Loss: 1.206 |  Val. Acc: 46.35%\n",
      "\tTrain Loss: -0.840 | Train Acc: 79.63%\n",
      "\t Val. Loss: 1.236 |  Val. Acc: 46.04%\n",
      "\tTrain Loss: -0.842 | Train Acc: 79.68%\n",
      "\t Val. Loss: 1.218 |  Val. Acc: 45.62%\n",
      "\tTrain Loss: -0.842 | Train Acc: 79.70%\n",
      "\t Val. Loss: 1.240 |  Val. Acc: 46.51%\n",
      "\tTrain Loss: -0.843 | Train Acc: 79.69%\n",
      "\t Val. Loss: 1.264 |  Val. Acc: 45.78%\n",
      "\tTrain Loss: -0.843 | Train Acc: 79.77%\n",
      "\t Val. Loss: 1.271 |  Val. Acc: 45.78%\n",
      "\tTrain Loss: -0.844 | Train Acc: 79.70%\n",
      "\t Val. Loss: 1.305 |  Val. Acc: 45.47%\n",
      "\tTrain Loss: -0.844 | Train Acc: 79.70%\n",
      "\t Val. Loss: 1.301 |  Val. Acc: 45.62%\n",
      "\tTrain Loss: -0.845 | Train Acc: 79.69%\n",
      "\t Val. Loss: 1.287 |  Val. Acc: 45.78%\n",
      "\tTrain Loss: -0.845 | Train Acc: 79.70%\n",
      "\t Val. Loss: 1.312 |  Val. Acc: 45.62%\n",
      "\tTrain Loss: -0.845 | Train Acc: 79.70%\n",
      "\t Val. Loss: 1.317 |  Val. Acc: 45.31%\n",
      "\tTrain Loss: -0.846 | Train Acc: 79.69%\n",
      "\t Val. Loss: 1.306 |  Val. Acc: 45.73%\n",
      "\tTrain Loss: -0.846 | Train Acc: 79.70%\n",
      "\t Val. Loss: 1.295 |  Val. Acc: 45.62%\n",
      "\tTrain Loss: -0.846 | Train Acc: 79.62%\n",
      "\t Val. Loss: 1.335 |  Val. Acc: 45.31%\n",
      "\tTrain Loss: -0.843 | Train Acc: 79.63%\n",
      "\t Val. Loss: 1.263 |  Val. Acc: 45.68%\n",
      "\tTrain Loss: -0.839 | Train Acc: 79.41%\n",
      "\t Val. Loss: 1.183 |  Val. Acc: 46.09%\n",
      "\tTrain Loss: -0.821 | Train Acc: 78.99%\n",
      "\t Val. Loss: 1.143 |  Val. Acc: 43.18%\n",
      "\tTrain Loss: -0.816 | Train Acc: 78.69%\n",
      "\t Val. Loss: 1.126 |  Val. Acc: 44.69%\n",
      "\tTrain Loss: -0.822 | Train Acc: 78.90%\n",
      "\t Val. Loss: 1.236 |  Val. Acc: 45.89%\n",
      "\tTrain Loss: -0.825 | Train Acc: 79.06%\n",
      "\t Val. Loss: 1.228 |  Val. Acc: 45.16%\n",
      "\tTrain Loss: -0.794 | Train Acc: 78.65%\n",
      "\t Val. Loss: 1.268 |  Val. Acc: 43.28%\n",
      "\tTrain Loss: -0.792 | Train Acc: 78.11%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 44.53%\n",
      "\tTrain Loss: -0.806 | Train Acc: 78.62%\n",
      "\t Val. Loss: 1.247 |  Val. Acc: 44.48%\n",
      "\tTrain Loss: -0.827 | Train Acc: 79.33%\n",
      "\t Val. Loss: 1.209 |  Val. Acc: 44.17%\n",
      "\tTrain Loss: -0.837 | Train Acc: 79.56%\n",
      "\t Val. Loss: 1.211 |  Val. Acc: 44.06%\n",
      "\tTrain Loss: -0.844 | Train Acc: 79.76%\n",
      "\t Val. Loss: 1.267 |  Val. Acc: 44.38%\n",
      "\tTrain Loss: -0.846 | Train Acc: 79.76%\n",
      "\t Val. Loss: 1.322 |  Val. Acc: 44.22%\n",
      "\tTrain Loss: -0.847 | Train Acc: 79.78%\n",
      "\t Val. Loss: 1.348 |  Val. Acc: 43.91%\n",
      "\tTrain Loss: -0.848 | Train Acc: 79.76%\n",
      "\t Val. Loss: 1.370 |  Val. Acc: 43.59%\n",
      "\tTrain Loss: -0.849 | Train Acc: 79.76%\n",
      "\t Val. Loss: 1.388 |  Val. Acc: 43.33%\n",
      "\tTrain Loss: -0.849 | Train Acc: 79.77%\n",
      "\t Val. Loss: 1.401 |  Val. Acc: 43.33%\n",
      "\tTrain Loss: -0.849 | Train Acc: 79.77%\n",
      "\t Val. Loss: 1.414 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: -0.850 | Train Acc: 79.76%\n",
      "\t Val. Loss: 1.419 |  Val. Acc: 42.71%\n",
      "\tTrain Loss: -0.851 | Train Acc: 79.82%\n",
      "\t Val. Loss: 1.424 |  Val. Acc: 43.49%\n",
      "\tTrain Loss: -0.851 | Train Acc: 79.76%\n",
      "\t Val. Loss: 1.423 |  Val. Acc: 43.49%\n",
      "\tTrain Loss: -0.852 | Train Acc: 79.82%\n",
      "\t Val. Loss: 1.434 |  Val. Acc: 43.33%\n",
      "\tTrain Loss: -0.851 | Train Acc: 79.84%\n",
      "\t Val. Loss: 1.438 |  Val. Acc: 43.65%\n",
      "\tTrain Loss: -0.852 | Train Acc: 79.76%\n",
      "\t Val. Loss: 1.436 |  Val. Acc: 43.65%\n",
      "\tTrain Loss: -0.851 | Train Acc: 79.86%\n",
      "\t Val. Loss: 1.451 |  Val. Acc: 43.33%\n",
      "\tTrain Loss: -0.852 | Train Acc: 79.84%\n",
      "\t Val. Loss: 1.449 |  Val. Acc: 43.65%\n",
      "\tTrain Loss: -0.852 | Train Acc: 79.83%\n",
      "\t Val. Loss: 1.448 |  Val. Acc: 43.49%\n",
      "\tTrain Loss: -0.852 | Train Acc: 79.84%\n",
      "\t Val. Loss: 1.448 |  Val. Acc: 43.65%\n",
      "\tTrain Loss: -0.852 | Train Acc: 79.84%\n",
      "\t Val. Loss: 1.447 |  Val. Acc: 43.49%\n",
      "\tTrain Loss: -0.853 | Train Acc: 79.84%\n",
      "\t Val. Loss: 1.454 |  Val. Acc: 43.18%\n",
      "\tTrain Loss: -0.853 | Train Acc: 79.83%\n",
      "\t Val. Loss: 1.453 |  Val. Acc: 43.49%\n",
      "\tTrain Loss: -0.852 | Train Acc: 79.86%\n",
      "\t Val. Loss: 1.446 |  Val. Acc: 43.33%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    #train the model\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    #evaluate the model\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177dc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
