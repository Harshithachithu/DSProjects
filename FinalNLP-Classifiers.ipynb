{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc2486a",
   "metadata": {},
   "source": [
    "# Importing Important Libraraies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d97f025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re,string\n",
    "\n",
    "#For preprocessing of text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#For building classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975d68e",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d636c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_load(filepath):\n",
    "    df = pd.read_csv(filepath, sep='\\t', header=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d364ec",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae3d7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert contrcations to normal meaningful words\n",
    "# Dictionary is taken from kaggle\n",
    "contractions_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                    \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n",
    "                    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
    "                    \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                    \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \n",
    "                    \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                    \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                    \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                    \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "                    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n",
    "                    \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                    \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n",
    "                    \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
    "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
    "                    \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n",
    "                    \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \n",
    "                    \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "                    \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
    "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n",
    "                    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
    "                    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                    \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                    \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "def convert_contraction_to_full(tweet):\n",
    "    if type(tweet) is str:\n",
    "        for key in contractions_dict:\n",
    "            value = contractions_dict[key]\n",
    "            tweet = tweet.replace(key, value)\n",
    "        return tweet\n",
    "    else:\n",
    "        return tweet\n",
    "    \n",
    "\n",
    "def preprocess_tweets(tweet):\n",
    "    \n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #Remove numbers\n",
    "    tweet = re.sub(r'\\b([0-9.]+)\\b', \" \", tweet)\n",
    "    \n",
    "    #Replace usermentions\n",
    "    r = re.findall(\"@[\\w]*\", tweet)\n",
    "    for word in r:\n",
    "        tweet = re.sub(word, \"<usermention>\", tweet)\n",
    "    \n",
    "    #Replace hashtags\n",
    "    tweet = re.sub(r'#([a-z0-9]+)(?:\\b|$)', \"<hashtag>\", tweet)\n",
    "\n",
    "    #Remove alphanumeric\n",
    "    tweet = re.sub(r'([^a-z0-9\\s\\<\\>]|[\\s]{2,})',\" \", tweet)\n",
    "    \n",
    "    #Remove punctuations\n",
    "    tweet = re.sub(r'[#\\(\\)\\[\\]\\{\\}\\.!\\?:\\-]',\" \", tweet)\n",
    "    \n",
    "    #remove shortwords\n",
    "    tweet = \" \".join([w for w in tweet.split() if len(w)>2])\n",
    "    \n",
    "    #Remove urls\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", tweet, flags = re.MULTILINE)\n",
    "    \n",
    "    #Remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    filtered_words = [word for word in tweet_tokens if word not in stopwords_english]\n",
    "    \n",
    "    #Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in filtered_words]\n",
    "    \n",
    "    return \" \". join(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28937425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>335104872099066692</td>\n",
       "      <td>positive</td>\n",
       "      <td>Felt privileged to play Foo Fighters songs on ...</td>\n",
       "      <td>felt privileged play foo fighters songs guitar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>796528524030124618</td>\n",
       "      <td>positive</td>\n",
       "      <td>@AaqibAfzaal Pakistan may be an Islamic countr...</td>\n",
       "      <td>&lt; usermention &gt; pakistan may islamic country d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>760964834217238632</td>\n",
       "      <td>positive</td>\n",
       "      <td>Happy Birthday to the coolest golfer in Bali! ...</td>\n",
       "      <td>happy birthday cool golfer bali &lt; usermention ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>147713180324524046</td>\n",
       "      <td>negative</td>\n",
       "      <td>@SimpplyA TMILLS is going to Tucson! But the 2...</td>\n",
       "      <td>&lt; usermention &gt; tmills going tucson 29th thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>732302280474120023</td>\n",
       "      <td>negative</td>\n",
       "      <td>Hmmmmm where are the #BlackLivesMatter when ma...</td>\n",
       "      <td>hmmmmm &lt; hashtag &gt; matters like rise kids disg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id     label  \\\n",
       "0  335104872099066692  positive   \n",
       "1  796528524030124618  positive   \n",
       "2  760964834217238632  positive   \n",
       "3  147713180324524046  negative   \n",
       "4  732302280474120023  negative   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  Felt privileged to play Foo Fighters songs on ...   \n",
       "1  @AaqibAfzaal Pakistan may be an Islamic countr...   \n",
       "2  Happy Birthday to the coolest golfer in Bali! ...   \n",
       "3  @SimpplyA TMILLS is going to Tucson! But the 2...   \n",
       "4  Hmmmmm where are the #BlackLivesMatter when ma...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  felt privileged play foo fighters songs guitar...  \n",
       "1  < usermention > pakistan may islamic country d...  \n",
       "2  happy birthday cool golfer bali < usermention ...  \n",
       "3  < usermention > tmills going tucson 29th thursday  \n",
       "4  hmmmmm < hashtag > matters like rise kids disg...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset_load(r\"C:/Users/harsh/Downloads/Warwick/NLP/Assignment/semeval-tweets.tar/semeval-tweets/twitter-training-data.txt\")\n",
    "df_test1 = dataset_load(r\"C:/Users/harsh/Downloads/Warwick/NLP/Assignment/semeval-tweets.tar/semeval-tweets/twitter-test1.txt\")\n",
    "df_test2 = dataset_load(r\"C:/Users/harsh/Downloads/Warwick/NLP/Assignment/semeval-tweets.tar/semeval-tweets/twitter-test2.txt\")\n",
    "df_test3 = dataset_load(r\"C:/Users/harsh/Downloads/Warwick/NLP/Assignment/semeval-tweets.tar/semeval-tweets/twitter-test3.txt\")\n",
    "\n",
    "df = df.rename({0: 'Id', 1: 'label' , 2: 'tweet'}, axis='columns')\n",
    "df['clean_tweet'] = np.vectorize(convert_contraction_to_full)(df['tweet'])\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: preprocess_tweets(x))\n",
    "\n",
    "df_test1 = df_test1.rename({0: 'Id', 1: 'label' , 2: 'tweet'}, axis='columns')\n",
    "df_test1['clean_tweet'] = np.vectorize(convert_contraction_to_full)(df_test1['tweet'])\n",
    "df_test1['clean_tweet'] = df_test1['clean_tweet'].apply(lambda x: preprocess_tweets(x))\n",
    "\n",
    "df_test2 = df_test2.rename({0: 'Id', 1: 'label' , 2: 'tweet'}, axis='columns')\n",
    "df_test2['clean_tweet'] = np.vectorize(convert_contraction_to_full)(df_test2['tweet'])\n",
    "df_test2['clean_tweet'] = df_test2['clean_tweet'].apply(lambda x: preprocess_tweets(x))\n",
    "\n",
    "df_test3 = df_test3.rename({0: 'Id', 1: 'label' , 2: 'tweet'}, axis='columns')\n",
    "df_test3['clean_tweet'] = np.vectorize(convert_contraction_to_full)(df_test3['tweet'])\n",
    "df_test3['clean_tweet'] = df_test3['clean_tweet'].apply(lambda x: preprocess_tweets(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b33395",
   "metadata": {},
   "source": [
    "## Vectorizing Tokens and Apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a88235ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - For twitter-training-data.text the f1score is: 0.6192223280922295\n",
      "Naive Bayes - For twitter-test1.text the f1score is:  0.5175122921229576\n",
      "Naive Bayes - For twitter-test2.text the f1score is:  0.5195061728395061\n",
      "Naive Bayes - For twitter-test3.text the f1score is:  0.5031187467640597\n"
     ]
    }
   ],
   "source": [
    "# Classsifier 1 - Naive Bayes\n",
    "#Use BOW for feature extraction\n",
    "\n",
    "bow_vectorizer= CountVectorizer(max_df=0.90, min_df=2,max_features =3000,stop_words='english')\n",
    "bow = bow_vectorizer.fit_transform(df['clean_tweet'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(bow,df['label'])\n",
    "predicted_naive = classifier.predict(bow)\n",
    "f1score = f1_score(df['label'], predicted_naive, average = 'macro', labels = ['positive', 'negative'])\n",
    "print('Naive Bayes - For twitter-training-data.text the f1score is:', f1score)\n",
    "\n",
    "def model_predict(df):\n",
    "    pred_value = classifier.predict(bow_vectorizer.transform(df['clean_tweet']))\n",
    "    f1score = f1_score(df['label'], pred_value, average = 'macro', labels = ['positive', 'negative'])\n",
    "    return f1score\n",
    "\n",
    "print('Naive Bayes - For twitter-test1.text the f1score is: ', model_predict(df_test1))\n",
    "print('Naive Bayes - For twitter-test2.text the f1score is: ', model_predict(df_test2))\n",
    "print('Naive Bayes - For twitter-test3.text the f1score is: ', model_predict(df_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6468133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - For twitter-training-data.text the f1score is:  0.6174036774232595\n",
      "SVM - For twitter-test1.text the f1score is:  0.5204917680621435\n",
      "SVM - For twitter-test2.text the f1score is:  0.5256331964728098\n",
      "SVM - For twitter-test3.text the f1score is:  0.4969621357352241\n"
     ]
    }
   ],
   "source": [
    "#Classifier 2 - SVM\n",
    "#TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.90, min_df=2,max_features =3000,stop_words='english')\n",
    "vec = vectorizer.fit_transform(df['clean_tweet'])\n",
    "\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(vec, df['label'])\n",
    "y_pred = svclassifier.predict(vec)\n",
    "f1score = f1_score(df['label'], y_pred, average = 'macro', labels = ['positive', 'negative'])\n",
    "print('SVM - For twitter-training-data.text the f1score is: ', f1score)\n",
    "\n",
    "def model_predict(df):\n",
    "    pred_value = svclassifier.predict(vectorizer.transform(df['clean_tweet']))\n",
    "    f1score = f1_score(df['label'], pred_value, average = 'macro', labels = ['positive', 'negative'])\n",
    "    return f1score\n",
    "\n",
    "print('SVM - For twitter-test1.text the f1score is: ', model_predict(df_test1))\n",
    "print('SVM - For twitter-test2.text the f1score is: ', model_predict(df_test2))\n",
    "print('SVM - For twitter-test3.text the f1score is: ', model_predict(df_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451493f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
